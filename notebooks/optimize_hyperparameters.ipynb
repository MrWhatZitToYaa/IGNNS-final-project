{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from bcnf.gp_minimize.gp_minimize import gp_minimize_fixed, save_checkpoint\n",
    "from bcnf.simulation.physics import get_data\n",
    "from bcnf.models.cnf import CondRealNVP\n",
    "from bcnf.models.feature_network import FullyConnectedFeatureNetwork\n",
    "from bcnf.eval.crossvalidate import cross_validate\n",
    "from bcnf.errors import TrainingDivergedError\n",
    "from bcnf.utils import get_dir\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: scikit-optimize is not maintained anymore and this is a quick fix to make it work\n",
    "# https://github.com/scikit-optimize/scikit-optimize/issues/1171#:~:text=To%20avoid%20this%20error%20in%20existing%20code%2C%20use%20int%20by%20itself.\n",
    "np.int = np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and load the progress in the Huggingface repository\n",
    "checkpoint_file = os.path.join(get_dir(\"models\", \"bcnf-models\", \"hyperparameter_optimization\", \"stage_2\", create=True), 'checkpoint_improved.pkl')\n",
    "metrics_dir = get_dir(\"models\", \"bcnf-models\", \"hyperparameter_optimization\", \"stage_2\", \"metrics\", create=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1034.73it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(\n",
    "    T=1.0,\n",
    "    dt=1 / 30,  # 30 fps\n",
    "    N=2_000,\n",
    "    break_on_impact=False  # Avoid discontinuities in the data\n",
    ")\n",
    "\n",
    "X_tensor = torch.Tensor(X.reshape(X.shape[0], -1))\n",
    "y_tensor = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = y_tensor.shape[1]\n",
    "feature_size = X_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters have worked well in pilot experiments\n",
    "optimizer_kwargs = {\n",
    "    \"lr\": 2e-4\n",
    "}\n",
    "\n",
    "lr_scheduler_kwargs = {\n",
    "    \"mode\": \"min\",\n",
    "    \"factor\": 0.5,\n",
    "    \"patience\": 250,\n",
    "    \"threshold_mode\": \"abs\",\n",
    "    \"threshold\": 1e-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "search_spaces = {\n",
    "    'condition_size': Integer(1, 2048),\n",
    "    'model_nested_size': Integer(16, 1024),\n",
    "    'model_nested_layers': Integer(1, 8),\n",
    "    'model_n_blocks': Integer(4, 32),\n",
    "    'model_act_norm': Categorical([True, False]),\n",
    "    'model_dropout': Real(0.0, 0.5),\n",
    "    'feature_network_hidden_size': Integer(16, 256),\n",
    "    'feature_network_hidden_layers': Integer(0, 16),\n",
    "    'feature_network_dropout': Real(0.0, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_index(name: str, search_spaces: dict[str, Real | Integer | Categorical]):\n",
    "    \"\"\"\n",
    "    Get the index of a parameter in the search space to match the order of the parameters in the optimization function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of the parameter.\n",
    "    search_spaces : dict[str, Real | Integer | Categorical]\n",
    "        The search space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the parameter in the search space.\n",
    "    \"\"\"\n",
    "    return list(search_spaces.keys()).index(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_parameters(params: list):\n",
    "    print({k: v for k, v in zip(search_spaces.keys(), params)})\n",
    "\n",
    "    # Catch training errors (such as divergence) to filter out bad parameter sets and speed up the optimization\n",
    "    try:\n",
    "        # Use cross-validation to estimate the performance of the model\n",
    "        fold_metrics = cross_validate(\n",
    "            model_class=CondRealNVP,\n",
    "            model_kwargs={\n",
    "                \"size\": model_size,\n",
    "                \"nested_sizes\": [params[param_index('model_nested_size', search_spaces)]] * params[param_index('model_nested_layers', search_spaces)],\n",
    "                \"n_blocks\": params[param_index('model_n_blocks', search_spaces)],\n",
    "                \"n_conditions\": params[param_index('condition_size', search_spaces)],\n",
    "                \"act_norm\": params[param_index('model_act_norm', search_spaces)],\n",
    "                \"dropout\": params[param_index('model_dropout', search_spaces)],\n",
    "            },\n",
    "            feature_network_class=FullyConnectedFeatureNetwork,\n",
    "            feature_network_kwargs={\n",
    "                \"sizes\": [feature_size]\n",
    "                    + [params[param_index('feature_network_hidden_size', search_spaces)]] * params[param_index('feature_network_hidden_layers', search_spaces)]\n",
    "                    + [params[param_index('condition_size', search_spaces)]],\n",
    "                \"dropout\": params[param_index('feature_network_dropout', search_spaces)],    \n",
    "            },\n",
    "            optimizer_class=torch.optim.Adam,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler_class=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            X=X_tensor,\n",
    "            y=y_tensor,\n",
    "            n_epochs=50_000,\n",
    "            val_loss_patience=500,\n",
    "            val_loss_tolerance=1e-1,  # Improvements to treat as significant\n",
    "            val_loss_tolerance_mode=\"abs\",\n",
    "            timeout=60 * 60,  # 1 hour\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            verbose=True,  # Print the progress\n",
    "            n_splits=3,\n",
    "            errors=\"raise\"  # Raise errors to stop the optimization\n",
    "        )\n",
    "\n",
    "        # Save the loss_history and metrics for analysis\n",
    "        with open(os.path.join(metrics_dir, f'params_{\"_\".join([str(p) for p in params])}.pkl'), 'wb') as f:\n",
    "            pickle.dump(fold_metrics, f)\n",
    "\n",
    "    except TrainingDivergedError as e:\n",
    "        print(e)\n",
    "        return 100  # A big number (bad score) to avoid this parameter set\n",
    "\n",
    "    # Print the average validation loss and its standard deviation during optimization\n",
    "    val_loss_list = [r['val_loss'][1] for r in fold_metrics]  # each val_loss value is a tuple (epoch, loss)\n",
    "    print(f'Val Loss: {np.mean(val_loss_list):.4f} ± {np.std(val_loss_list):.4f}')\n",
    "\n",
    "    # Return the upper confidence bound of the validation loss\n",
    "    # This encourages the optimization to find good AND reliable parameter sets\n",
    "    return np.mean(val_loss_list) + np.std(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(result):\n",
    "    \"\"\"\n",
    "    Save the result of the optimization to a checkpoint file.\n",
    "    Used as a callback in the optimization function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : OptimizeResult\n",
    "        The result of the optimization.\n",
    "    \"\"\"\n",
    "    # Safely write the result to a temporary file first without overwriting the checkpoint file\n",
    "    with open(checkpoint_file + \".tmp\", 'wb') as f:\n",
    "        # Ignore\n",
    "        # - result['specs']['args']['func']\n",
    "        # - result['specs']['args']['callback']\n",
    "        # because it causes problems when reading somewhere else\n",
    "        result_no_func = copy.deepcopy(result)\n",
    "        del result_no_func['specs']['args']['func']\n",
    "        del result_no_func['specs']['args']['callback']\n",
    "        pickle.dump(result_no_func, f)\n",
    "\n",
    "    # Delete the old checkpoint file and rename the temporary file\n",
    "    shutil.move(checkpoint_file + \".tmp\", checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numer of random initial points to explore the search space\n",
    "N_STEPS_INIT = 30\n",
    "\n",
    "# Number of iterations to run the optimization in total\n",
    "N_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint if it exists\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(f'Loading checkpoint from {checkpoint_file}')\n",
    "\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "\n",
    "        # Re-assign the function and callback because they are not picklable\n",
    "        checkpoint['specs']['args']['func'] = score_parameters\n",
    "        checkpoint['specs']['args']['callback'] = save_checkpoint\n",
    "\n",
    "    print(f'Resuming from iteration {len(checkpoint.x_iters)}')\n",
    "\n",
    "    n_initial_points = max(0, N_STEPS_INIT - len(checkpoint.x_iters))\n",
    "    n_calls_remaining = max(0, N_STEPS - len(checkpoint.x_iters))\n",
    "    x0 = checkpoint.x_iters\n",
    "    y0 = checkpoint.func_vals\n",
    "else:\n",
    "    print('No checkpoint found. Starting new optimization')\n",
    "    checkpoint = None\n",
    "\n",
    "    n_initial_points = N_STEPS_INIT\n",
    "    n_calls_remaining = N_STEPS\n",
    "\n",
    "    x0 = None\n",
    "    y0 = None\n",
    "    \n",
    "print(f'Running with {n_initial_points} initial points and {n_calls_remaining} remaining iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoint.pkl\n",
      "Resuming from iteration 95\n",
      "Running with 0 initial points and 5 remaining iterations\n",
      "0 initial points will be randomly generated\n",
      "Iteration No: 1 started. Searching for the next optimal point.\n",
      "Telling optimizer about 95 initial points\n",
      "Iteration No: 1 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0591\n",
      "Function value obtained: -20.4135\n",
      "Current minimum: -53.3666\n",
      "Iteration No: 2 started. Searching for the next optimal point.\n",
      "[2048, 256, 32, False, 2048, 9, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: -44.9218 - Val: 154.4365 (avg: 133.3613, min: -19.0563) | lr: 1.00e-04 - Patience: 500/500:   2%|▏         | 1232/50000 [04:07<2:43:14,  4.98it/s]\n",
      "Train: -22.6586 - Val: -19.2061 (avg: -18.1062, min: -18.7781) | lr: 2.00e-04 - Patience: 27/500:   2%|▏         | 753/50000 [02:32<2:45:51,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in fold 1: Loss exploded to 263129382912.0 at epoch 753.3333333333334\n",
      "Loss exploded to 263129382912.0 at epoch 753.3333333333334\n",
      "Iteration No: 2 ended. Search finished for the next optimal point.\n",
      "Time taken: 402.0372\n",
      "Function value obtained: 100.0000\n",
      "Current minimum: -53.3666\n",
      "Iteration No: 3 started. Searching for the next optimal point.\n",
      "[1588, 161, 23, True, 31, 3, 0.3779247177629724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: -55.3583 - Val: -53.6495 (avg: -53.6480, min: -53.5591) | lr: 1.95e-07 - Patience: 500/500:  26%|██▌       | 12977/50000 [30:11<1:26:07,  7.16it/s]\n",
      "Train: -54.5522 - Val: -53.8717 (avg: -53.8755, min: -53.8081) | lr: 1.95e-07 - Patience: 500/500:  26%|██▌       | 12950/50000 [30:28<1:27:12,  7.08it/s]\n",
      "Train: -54.8296 - Val: -53.0909 (avg: -53.0882, min: -53.0344) | lr: 1.95e-07 - Patience: 500/500:  25%|██▍       | 12412/50000 [29:13<1:28:29,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: -53.7204 ± 0.4293\n",
      "Iteration No: 3 ended. Search finished for the next optimal point.\n",
      "Time taken: 5395.7605\n",
      "Function value obtained: -53.2911\n",
      "Current minimum: -53.3666\n",
      "Iteration No: 4 started. Searching for the next optimal point.\n",
      "[1667, 157, 23, True, 16, 3, 0.3886034390791541]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: -51.9401 - Val: -47.6704 (avg: -47.6710, min: -47.6082) | lr: 1.95e-07 - Patience: 500/500:  25%|██▌       | 12559/50000 [28:17<1:24:20,  7.40it/s]\n",
      "Train: -50.3264 - Val: -47.0274 (avg: -47.0168, min: -46.9642) | lr: 1.95e-07 - Patience: 500/500:  22%|██▏       | 11035/50000 [24:53<1:27:52,  7.39it/s]\n",
      "Train: -51.0544 - Val: -49.7665 (avg: -49.6845, min: -49.6970) | lr: 3.13e-06 - Patience: 12/500:  21%|██▏       | 10707/50000 [27:27<2:05:41,  5.21it/s] "
     ]
    }
   ],
   "source": [
    "# You might want to adjust the n_calls or other parameters based on the checkpoint\n",
    "result = gp_minimize_fixed(\n",
    "    func=score_parameters,\n",
    "    dimensions=search_spaces.values(),\n",
    "    n_initial_points=n_initial_points,  # Number of random points before starting the optimization\n",
    "    n_calls=n_calls_remaining,  # Number of iterations\n",
    "    random_state=2024_03_25,\n",
    "    verbose=True,\n",
    "    acq_func=\"EI\",  # Expected Improvement https://arxiv.org/pdf/1009.5419.pdf\n",
    "    initial_point_generator=\"halton\", # https://en.wikipedia.org/wiki/Halton_sequence\n",
    "    callback=save_checkpoint,\n",
    "    x0=x0,\n",
    "    y0=y0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
