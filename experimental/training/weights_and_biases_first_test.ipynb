{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n",
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_model = torch.cuda.get_device_name(device)\n",
    "print(gpu_model)\n",
    "print(type(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                      if not mirror.startswith(\"http://yann.lecun.com\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config can obviously be a yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    kernels=[16, 32, 48, 24, 16],\n",
    "    batch_size=128,\n",
    "    learning_rate=0.002,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to wandb one \"typically has a pipeline\"\n",
    "\n",
    "This is specifically an example for training a CNN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importatnt step is that everything gets done in the context of wandb.init. I don't understand the techicalities yet but the dude in the video claims that a seperate process is created. I don't know yet if he means the init or the whole training but i would assume the whole training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo\",\n",
    "                    config=hyperparameters):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader, criterion, optimizer, config)\n",
    "\n",
    "      # and test its final performance\n",
    "      test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not so interesting stuff cause not related to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    train, test = get_data(train=True), get_data(train=False)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = ConvNet(config.kernels, config.classes).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, train_loader, test_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(slice=5, train=True):\n",
    "    full_dataset = torchvision.datasets.MNIST(root=\".\",\n",
    "                                              train=train, \n",
    "                                              transform=transforms.ToTensor(),\n",
    "                                              download=True)\n",
    "    #  equiv to slicing with [::slice] \n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "      full_dataset, indices=range(0, len(full_dataset), slice))\n",
    "    \n",
    "    return sub_dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernels, classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(kernels[1], kernels[2], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(3 * 3 * kernels[2], 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.fc2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the start where wandb really starts coming into play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special attention to watch and log function here. It is especially useful for pytorch, logs gradient and parameters of model in frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for _, (images, labels) in enumerate(loader):\n",
    "\n",
    "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb.log expects a dictionary with strings as keys. These strings identify the objects being logged, which make up the values. You can also optionally log which step of training you're on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb.save saves the model paramters to disk on the wandb server. TODO: Find out what the maximum capacity of free storage is. This is convenient, since we can associate the model file with the training run easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX = Open neural network exchange format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test images: {correct / total:%}\")\n",
    "        \n",
    "        wandb.log({\"test_accuracy\": correct / total})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, images, \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ./wandb_artifacts/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path ./wandb_artifacts/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20240310_222030-el9bwey5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/balisticcnf/pytorch-demo/runs/el9bwey5' target=\"_blank\">true-fire-13</a></strong> to <a href='https://wandb.ai/balisticcnf/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/balisticcnf/pytorch-demo' target=\"_blank\">https://wandb.ai/balisticcnf/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/balisticcnf/pytorch-demo/runs/el9bwey5' target=\"_blank\">https://wandb.ai/balisticcnf/pytorch-demo/runs/el9bwey5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [01:02<00:00, 158831.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 66412112.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:20<00:00, 82013.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 14476085.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=432, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d61deffcad8456f9e820f866969396d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03072 examples: 0.824\n",
      "Loss after 06272 examples: 0.682\n",
      "Loss after 09472 examples: 0.359\n",
      "Loss after 12640 examples: 0.361\n",
      "Loss after 15840 examples: 0.151\n",
      "Loss after 19040 examples: 0.266\n",
      "Loss after 22240 examples: 0.245\n",
      "Loss after 25408 examples: 0.184\n",
      "Loss after 28608 examples: 0.147\n",
      "Loss after 31808 examples: 0.113\n",
      "Loss after 35008 examples: 0.084\n",
      "Loss after 38176 examples: 0.078\n",
      "Loss after 41376 examples: 0.058\n",
      "Loss after 44576 examples: 0.052\n",
      "Loss after 47776 examples: 0.124\n",
      "Loss after 50944 examples: 0.030\n",
      "Loss after 54144 examples: 0.046\n",
      "Loss after 57344 examples: 0.105\n",
      "Loss after 60512 examples: 0.023\n",
      "Loss after 63712 examples: 0.032\n",
      "Loss after 66912 examples: 0.039\n",
      "Loss after 70112 examples: 0.066\n",
      "Loss after 73280 examples: 0.031\n",
      "Loss after 76480 examples: 0.023\n",
      "Loss after 79680 examples: 0.043\n",
      "Loss after 82880 examples: 0.016\n",
      "Loss after 86048 examples: 0.010\n",
      "Loss after 89248 examples: 0.036\n",
      "Loss after 92448 examples: 0.044\n",
      "Loss after 95648 examples: 0.165\n",
      "Loss after 98816 examples: 0.008\n",
      "Loss after 102016 examples: 0.031\n",
      "Loss after 105216 examples: 0.010\n",
      "Loss after 108384 examples: 0.008\n",
      "Loss after 111584 examples: 0.012\n",
      "Loss after 114784 examples: 0.031\n",
      "Loss after 117984 examples: 0.016\n",
      "Loss after 121152 examples: 0.043\n",
      "Loss after 124352 examples: 0.059\n",
      "Loss after 127552 examples: 0.015\n",
      "Loss after 130752 examples: 0.029\n",
      "Loss after 133920 examples: 0.005\n",
      "Loss after 137120 examples: 0.015\n",
      "Loss after 140320 examples: 0.003\n",
      "Loss after 143520 examples: 0.005\n",
      "Loss after 146688 examples: 0.031\n",
      "Loss after 149888 examples: 0.001\n",
      "Loss after 153088 examples: 0.003\n",
      "Loss after 156256 examples: 0.002\n",
      "Loss after 159456 examples: 0.017\n",
      "Loss after 162656 examples: 0.001\n",
      "Loss after 165856 examples: 0.017\n",
      "Loss after 169024 examples: 0.012\n",
      "Loss after 172224 examples: 0.018\n",
      "Loss after 175424 examples: 0.000\n",
      "Loss after 178624 examples: 0.043\n",
      "Loss after 181792 examples: 0.003\n",
      "Loss after 184992 examples: 0.005\n",
      "Loss after 188192 examples: 0.001\n",
      "Loss after 191392 examples: 0.023\n",
      "Loss after 194560 examples: 0.003\n",
      "Loss after 197760 examples: 0.019\n",
      "Loss after 200960 examples: 0.003\n",
      "Loss after 204128 examples: 0.002\n",
      "Loss after 207328 examples: 0.002\n",
      "Loss after 210528 examples: 0.023\n",
      "Loss after 213728 examples: 0.000\n",
      "Loss after 216896 examples: 0.029\n",
      "Loss after 220096 examples: 0.003\n",
      "Loss after 223296 examples: 0.000\n",
      "Loss after 226496 examples: 0.001\n",
      "Loss after 229664 examples: 0.005\n",
      "Loss after 232864 examples: 0.013\n",
      "Loss after 236064 examples: 0.002\n",
      "Loss after 239264 examples: 0.008\n",
      "Loss after 242432 examples: 0.003\n",
      "Loss after 245632 examples: 0.001\n",
      "Loss after 248832 examples: 0.001\n",
      "Loss after 252000 examples: 0.001\n",
      "Loss after 255200 examples: 0.004\n",
      "Loss after 258400 examples: 0.031\n",
      "Loss after 261600 examples: 0.010\n",
      "Loss after 264768 examples: 0.000\n",
      "Loss after 267968 examples: 0.002\n",
      "Loss after 271168 examples: 0.001\n",
      "Loss after 274368 examples: 0.046\n",
      "Loss after 277536 examples: 0.001\n",
      "Loss after 280736 examples: 0.005\n",
      "Loss after 283936 examples: 0.001\n",
      "Loss after 287136 examples: 0.000\n",
      "Loss after 290304 examples: 0.035\n",
      "Loss after 293504 examples: 0.002\n",
      "Loss after 296704 examples: 0.002\n",
      "Loss after 299904 examples: 0.017\n",
      "Loss after 303072 examples: 0.021\n",
      "Loss after 306272 examples: 0.000\n",
      "Loss after 309472 examples: 0.002\n",
      "Loss after 312640 examples: 0.003\n",
      "Loss after 315840 examples: 0.000\n",
      "Loss after 319040 examples: 0.010\n",
      "Loss after 322240 examples: 0.036\n",
      "Loss after 325408 examples: 0.009\n",
      "Loss after 328608 examples: 0.001\n",
      "Loss after 331808 examples: 0.000\n",
      "Loss after 335008 examples: 0.001\n",
      "Loss after 338176 examples: 0.002\n",
      "Loss after 341376 examples: 0.000\n",
      "Loss after 344576 examples: 0.001\n",
      "Loss after 347776 examples: 0.000\n",
      "Loss after 350944 examples: 0.000\n",
      "Loss after 354144 examples: 0.000\n",
      "Loss after 357344 examples: 0.000\n",
      "Loss after 360512 examples: 0.000\n",
      "Loss after 363712 examples: 0.001\n",
      "Loss after 366912 examples: 0.000\n",
      "Loss after 370112 examples: 0.000\n",
      "Loss after 373280 examples: 0.000\n",
      "Loss after 376480 examples: 0.000\n",
      "Loss after 379680 examples: 0.000\n",
      "Loss after 382880 examples: 0.000\n",
      "Loss after 386048 examples: 0.000\n",
      "Loss after 389248 examples: 0.000\n",
      "Loss after 392448 examples: 0.000\n",
      "Loss after 395648 examples: 0.000\n",
      "Loss after 398816 examples: 0.000\n",
      "Loss after 402016 examples: 0.000\n",
      "Loss after 405216 examples: 0.000\n",
      "Loss after 408384 examples: 0.000\n",
      "Loss after 411584 examples: 0.000\n",
      "Loss after 414784 examples: 0.000\n",
      "Loss after 417984 examples: 0.000\n",
      "Loss after 421152 examples: 0.000\n",
      "Loss after 424352 examples: 0.000\n",
      "Loss after 427552 examples: 0.000\n",
      "Loss after 430752 examples: 0.000\n",
      "Loss after 433920 examples: 0.000\n",
      "Loss after 437120 examples: 0.000\n",
      "Loss after 440320 examples: 0.000\n",
      "Loss after 443520 examples: 0.000\n",
      "Loss after 446688 examples: 0.000\n",
      "Loss after 449888 examples: 0.000\n",
      "Loss after 453088 examples: 0.000\n",
      "Loss after 456256 examples: 0.000\n",
      "Loss after 459456 examples: 0.000\n",
      "Loss after 462656 examples: 0.000\n",
      "Loss after 465856 examples: 0.000\n",
      "Loss after 469024 examples: 0.000\n",
      "Loss after 472224 examples: 0.000\n",
      "Loss after 475424 examples: 0.000\n",
      "Loss after 478624 examples: 0.000\n",
      "Loss after 481792 examples: 0.000\n",
      "Loss after 484992 examples: 0.000\n",
      "Loss after 488192 examples: 0.000\n",
      "Loss after 491392 examples: 0.000\n",
      "Loss after 494560 examples: 0.000\n",
      "Loss after 497760 examples: 0.000\n",
      "Loss after 500960 examples: 0.000\n",
      "Loss after 504128 examples: 0.000\n",
      "Loss after 507328 examples: 0.000\n",
      "Loss after 510528 examples: 0.000\n",
      "Loss after 513728 examples: 0.000\n",
      "Loss after 516896 examples: 0.000\n",
      "Loss after 520096 examples: 0.000\n",
      "Loss after 523296 examples: 0.000\n",
      "Loss after 526496 examples: 0.000\n",
      "Loss after 529664 examples: 0.000\n",
      "Loss after 532864 examples: 0.000\n",
      "Loss after 536064 examples: 0.000\n",
      "Loss after 539264 examples: 0.000\n",
      "Loss after 542432 examples: 0.000\n",
      "Loss after 545632 examples: 0.000\n",
      "Loss after 548832 examples: 0.000\n",
      "Loss after 552000 examples: 0.000\n",
      "Loss after 555200 examples: 0.000\n",
      "Loss after 558400 examples: 0.000\n",
      "Loss after 561600 examples: 0.000\n",
      "Loss after 564768 examples: 0.000\n",
      "Loss after 567968 examples: 0.000\n",
      "Loss after 571168 examples: 0.000\n",
      "Loss after 574368 examples: 0.000\n",
      "Loss after 577536 examples: 0.000\n",
      "Loss after 580736 examples: 0.000\n",
      "Loss after 583936 examples: 0.000\n",
      "Loss after 587136 examples: 0.000\n",
      "Loss after 590304 examples: 0.000\n",
      "Loss after 593504 examples: 0.000\n",
      "Loss after 596704 examples: 0.000\n",
      "Loss after 599904 examples: 0.000\n",
      "Loss after 603072 examples: 0.000\n",
      "Loss after 606272 examples: 0.000\n",
      "Loss after 609472 examples: 0.000\n",
      "Loss after 612640 examples: 0.000\n",
      "Loss after 615840 examples: 0.000\n",
      "Loss after 619040 examples: 0.000\n",
      "Loss after 622240 examples: 0.000\n",
      "Loss after 625408 examples: 0.000\n",
      "Loss after 628608 examples: 0.000\n",
      "Loss after 631808 examples: 0.000\n",
      "Loss after 635008 examples: 0.000\n",
      "Loss after 638176 examples: 0.000\n",
      "Loss after 641376 examples: 0.000\n",
      "Loss after 644576 examples: 0.000\n",
      "Loss after 647776 examples: 0.000\n",
      "Loss after 650944 examples: 0.000\n",
      "Loss after 654144 examples: 0.000\n",
      "Loss after 657344 examples: 0.000\n",
      "Loss after 660512 examples: 0.000\n",
      "Loss after 663712 examples: 0.000\n",
      "Loss after 666912 examples: 0.000\n",
      "Loss after 670112 examples: 0.000\n",
      "Loss after 673280 examples: 0.000\n",
      "Loss after 676480 examples: 0.000\n",
      "Loss after 679680 examples: 0.000\n",
      "Loss after 682880 examples: 0.000\n",
      "Loss after 686048 examples: 0.000\n",
      "Loss after 689248 examples: 0.000\n",
      "Loss after 692448 examples: 0.000\n",
      "Loss after 695648 examples: 0.000\n",
      "Loss after 698816 examples: 0.000\n",
      "Loss after 702016 examples: 0.000\n",
      "Loss after 705216 examples: 0.000\n",
      "Loss after 708384 examples: 0.000\n",
      "Loss after 711584 examples: 0.000\n",
      "Loss after 714784 examples: 0.000\n",
      "Loss after 717984 examples: 0.000\n",
      "Loss after 721152 examples: 0.000\n",
      "Loss after 724352 examples: 0.000\n",
      "Loss after 727552 examples: 0.000\n",
      "Loss after 730752 examples: 0.000\n",
      "Loss after 733920 examples: 0.000\n",
      "Loss after 737120 examples: 0.000\n",
      "Loss after 740320 examples: 0.000\n",
      "Loss after 743520 examples: 0.000\n",
      "Loss after 746688 examples: 0.000\n",
      "Loss after 749888 examples: 0.000\n",
      "Loss after 753088 examples: 0.000\n",
      "Loss after 756256 examples: 0.000\n",
      "Loss after 759456 examples: 0.000\n",
      "Loss after 762656 examples: 0.000\n",
      "Loss after 765856 examples: 0.000\n",
      "Loss after 769024 examples: 0.000\n",
      "Loss after 772224 examples: 0.000\n",
      "Loss after 775424 examples: 0.000\n",
      "Loss after 778624 examples: 0.000\n",
      "Loss after 781792 examples: 0.000\n",
      "Loss after 784992 examples: 0.000\n",
      "Loss after 788192 examples: 0.000\n",
      "Loss after 791392 examples: 0.000\n",
      "Loss after 794560 examples: 0.000\n",
      "Loss after 797760 examples: 0.000\n",
      "Loss after 800960 examples: 0.000\n",
      "Loss after 804128 examples: 0.000\n",
      "Loss after 807328 examples: 0.000\n",
      "Loss after 810528 examples: 0.000\n",
      "Loss after 813728 examples: 0.000\n",
      "Loss after 816896 examples: 0.000\n",
      "Loss after 820096 examples: 0.000\n",
      "Loss after 823296 examples: 0.000\n",
      "Loss after 826496 examples: 0.000\n",
      "Loss after 829664 examples: 0.000\n",
      "Loss after 832864 examples: 0.000\n",
      "Loss after 836064 examples: 0.000\n",
      "Loss after 839264 examples: 0.000\n",
      "Loss after 842432 examples: 0.000\n",
      "Loss after 845632 examples: 0.000\n",
      "Loss after 848832 examples: 0.000\n",
      "Loss after 852000 examples: 0.000\n",
      "Loss after 855200 examples: 0.000\n",
      "Loss after 858400 examples: 0.000\n",
      "Loss after 861600 examples: 0.000\n",
      "Loss after 864768 examples: 0.000\n",
      "Loss after 867968 examples: 0.000\n",
      "Loss after 871168 examples: 0.000\n",
      "Loss after 874368 examples: 0.000\n",
      "Loss after 877536 examples: 0.000\n",
      "Loss after 880736 examples: 0.000\n",
      "Loss after 883936 examples: 0.000\n",
      "Loss after 887136 examples: 0.000\n",
      "Loss after 890304 examples: 0.000\n",
      "Loss after 893504 examples: 0.000\n",
      "Loss after 896704 examples: 0.000\n",
      "Loss after 899904 examples: 0.000\n",
      "Loss after 903072 examples: 0.000\n",
      "Loss after 906272 examples: 0.000\n",
      "Loss after 909472 examples: 0.000\n",
      "Loss after 912640 examples: 0.000\n",
      "Loss after 915840 examples: 0.000\n",
      "Loss after 919040 examples: 0.000\n",
      "Loss after 922240 examples: 0.000\n",
      "Loss after 925408 examples: 0.000\n",
      "Loss after 928608 examples: 0.000\n",
      "Loss after 931808 examples: 0.000\n",
      "Loss after 935008 examples: 0.000\n",
      "Loss after 938176 examples: 0.000\n",
      "Loss after 941376 examples: 0.000\n",
      "Loss after 944576 examples: 0.000\n",
      "Loss after 947776 examples: 0.000\n",
      "Loss after 950944 examples: 0.000\n",
      "Loss after 954144 examples: 0.000\n",
      "Loss after 957344 examples: 0.000\n",
      "Loss after 960512 examples: 0.000\n",
      "Loss after 963712 examples: 0.000\n",
      "Loss after 966912 examples: 0.000\n",
      "Loss after 970112 examples: 0.000\n",
      "Loss after 973280 examples: 0.000\n",
      "Loss after 976480 examples: 0.000\n",
      "Loss after 979680 examples: 0.000\n",
      "Loss after 982880 examples: 0.000\n",
      "Loss after 986048 examples: 0.000\n",
      "Loss after 989248 examples: 0.000\n",
      "Loss after 992448 examples: 0.000\n",
      "Loss after 995648 examples: 0.000\n",
      "Loss after 998816 examples: 0.000\n",
      "Loss after 1002016 examples: 0.000\n",
      "Loss after 1005216 examples: 0.000\n",
      "Loss after 1008384 examples: 0.000\n",
      "Loss after 1011584 examples: 0.000\n",
      "Loss after 1014784 examples: 0.000\n",
      "Loss after 1017984 examples: 0.000\n",
      "Loss after 1021152 examples: 0.000\n",
      "Loss after 1024352 examples: 0.000\n",
      "Loss after 1027552 examples: 0.000\n",
      "Loss after 1030752 examples: 0.000\n",
      "Loss after 1033920 examples: 0.000\n",
      "Loss after 1037120 examples: 0.000\n",
      "Loss after 1040320 examples: 0.000\n",
      "Loss after 1043520 examples: 0.000\n",
      "Loss after 1046688 examples: 0.000\n",
      "Loss after 1049888 examples: 0.000\n",
      "Loss after 1053088 examples: 0.000\n",
      "Loss after 1056256 examples: 0.000\n",
      "Loss after 1059456 examples: 0.000\n",
      "Loss after 1062656 examples: 0.000\n",
      "Loss after 1065856 examples: 0.000\n",
      "Loss after 1069024 examples: 0.000\n",
      "Loss after 1072224 examples: 0.000\n",
      "Loss after 1075424 examples: 0.000\n",
      "Loss after 1078624 examples: 0.000\n",
      "Loss after 1081792 examples: 0.000\n",
      "Loss after 1084992 examples: 0.000\n",
      "Loss after 1088192 examples: 0.000\n",
      "Loss after 1091392 examples: 0.000\n",
      "Loss after 1094560 examples: 0.000\n",
      "Loss after 1097760 examples: 0.000\n",
      "Loss after 1100960 examples: 0.000\n",
      "Loss after 1104128 examples: 0.000\n",
      "Loss after 1107328 examples: 0.000\n",
      "Loss after 1110528 examples: 0.000\n",
      "Loss after 1113728 examples: 0.000\n",
      "Loss after 1116896 examples: 0.000\n",
      "Loss after 1120096 examples: 0.000\n",
      "Loss after 1123296 examples: 0.000\n",
      "Loss after 1126496 examples: 0.000\n",
      "Loss after 1129664 examples: 0.000\n",
      "Loss after 1132864 examples: 0.000\n",
      "Loss after 1136064 examples: 0.000\n",
      "Loss after 1139264 examples: 0.000\n",
      "Loss after 1142432 examples: 0.000\n",
      "Loss after 1145632 examples: 0.000\n",
      "Loss after 1148832 examples: 0.000\n",
      "Loss after 1152000 examples: 0.000\n",
      "Loss after 1155200 examples: 0.000\n",
      "Loss after 1158400 examples: 0.000\n",
      "Loss after 1161600 examples: 0.000\n",
      "Loss after 1164768 examples: 0.000\n",
      "Loss after 1167968 examples: 0.000\n",
      "Loss after 1171168 examples: 0.000\n",
      "Loss after 1174368 examples: 0.000\n",
      "Loss after 1177536 examples: 0.000\n",
      "Loss after 1180736 examples: 0.000\n",
      "Loss after 1183936 examples: 0.000\n",
      "Loss after 1187136 examples: 0.000\n",
      "Loss after 1190304 examples: 0.000\n",
      "Loss after 1193504 examples: 0.000\n",
      "Loss after 1196704 examples: 0.000\n",
      "Loss after 1199904 examples: 0.000\n",
      "Accuracy of the model on the 2000 test images: 98.550000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cb8784286e435fa940bb2fa8a138ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.442 MB of 0.442 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.0</td></tr><tr><td>test_accuracy</td><td>0.9855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-fire-13</strong> at: <a href='https://wandb.ai/balisticcnf/pytorch-demo/runs/el9bwey5' target=\"_blank\">https://wandb.ai/balisticcnf/pytorch-demo/runs/el9bwey5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20240310_222030-el9bwey5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
